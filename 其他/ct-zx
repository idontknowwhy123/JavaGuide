问题现象：
任务执行非常耗时，正常情况下只需要几百毫秒的任务，实际耗时十几秒，导致环境上任务执行不完
日志定位：1、是否是设备响应太慢，通过日志发现设备响应那块实际耗时大概只花了1S
          2、是否是因为同步锁等待导致线程阻塞，从而导致耗时严重，走查代码发现并没有非常耗时的等待锁操作
          3、通过top命令发现服务进程CPU冲高到了1400%
		     具体分析：3.1、通过jstack命令查看线程情况，发现GC线程比较耗CPU
			           3.2、查看GC日志，大概1秒执行一次FULL GC，FULL GC总共耗时大概是0.5s
					   3.3、通过jmap命令查看堆内存使用情况，发现存在很多相同的ItmNeInfo对象都放在很多List集合里面
					   3.4、查看代码，看哪些地方存在new ItmNeInfo()并存放在List集合的地方
					   3.5、发现有个方法每执行一个任务都会去调用它，这个方法每次执行时都会首先new一个ArrayList,然后从缓存中取出该类型所有网元对应
					        Json字符串，最后再将Json字符串转化为对象，现场环境上的网元实例数大概是2000多，任务是两万个，每5分钟粒度总共只执行完了6000
							多个任务，这样理论上每5分钟粒度会创建上千万个ItmNeInfo对象。
			 解决办法：每次缓存的时候不再将网元对象转化为字符串再进行存储了，而是直接缓存网元对象，这样每次从缓存取的时候是直接取的对象，使用的就是缓存           	              中原有的对象而不是通过字符串转化过来的对象
			 效果：现在Full GC的频率从之前的1s延长到现在7,8分钟一次，任务耗时从之前的十几秒缩减到现在的几百毫秒
			 
			 
			 
			 项目难点mps水平扩展性能数据上报，技术挑战（3种方案及优缺点）

弹缩的背景:

随着工作负载的不断加大，我们的应用程序需要不断的适应不同工作负载下的性能压力，这个时候我们需要使用k8s的pod水平自动伸缩技术来对pod的数量进行水平伸缩，性能压力较大时我们扩展pod数量，性能压力
较小时我们减小pod数量

技术难点：
   原有逻辑：uaf通过rest接口将性能数据上报给mps，mps首先将数据放到一个生产消费者队列，再慢慢去消费这些数据（包括阈值告警的判断，性能数据的上报等逻辑）
   原有逻辑在弹缩场景下存在的问题：在弹缩的场景下，如果存在mps对应的pod中途退出，那么上报到消息队列中的数据如果没有处理完，将会丢失
   
解决办法（3种方案及优缺点）

如何保证消息的顺序消费?（必须是先有告警，再有恢复，顺序不能错乱）
1、整体逻辑与性能数据上报逻辑一致，只不过为了保证同一个设备告警上报的顺序，我们需要保证同一个设备对应的告警消息在同一个分区(同一个分区里面的消息是顺序的)，我们在创建kafka消息时(ProducerRecord)，传入neId这个key(能够唯一标志某个设备)，
2、如果客户端发送失败可能会因为网络问题进行重试，重试可能会导致后面发送的消息最后排列到前面的消息前面去了，max.in.flight.requests.per.connection设置为1，这样在生产者尝试发送第一批消息时，如果消息还没发送完毕，就不会有其他的消息发送给kafka broker

kafka出现消息消息积压怎么办？

1）如果是Kafka消费能力不足，则可以考虑增加Topic的分区数，并且同时提升消费组的消费者数量，消费者数=分区数。（两者缺一不可）

2）如果是下游的数据处理不及时：提高每批次拉取的数量。批次拉取数据过少（拉取数据/处理时间<生产速度），使处理的数据小于生产的数据，也会造成数据积压。

遇到的问题？
性能数据入库失败抛出异常没有捕获到导致那一条数据处理完之后计数器countDownLatch没有进行countDown()减一操作，由于kafka轮询线程一波数据最长等待时间是3分钟（两拨数据异常就要处理6分钟），
这就导致轮询线程处理数据出现了阻塞现象，而uaf上报数据还是5分钟上报一次，这就导致上报到kafka中的性能数据积压严重，并不能实时入库。
解决办法:每条性能数据入库完之后在finally里面对计数器进行减一，保证数据处理完（无论是成功还是失败）都需要对计数器进行减一。
		 将轮询线程最长等待时间缩短为1分钟，保证不会因为一波性能数据处理异常阻塞后面的性能数据处理。

kafka的优点：
1、高吞吐:支持每秒百万级别的吞吐量
2、高可用:副本机制保证，当master副本挂掉以后，会从其他副本里面选举出一个master副本
3、可靠性：
4、分布式：支持消息的分区，通过kafka服务器和消费者机器的集群进行分布式消费


方案三（利用k8s的优雅停止pod机制）：表示当POD实例退出时(例如UME卸载，或者UME升级时旧版本的退出)，系统会先向容器内的主进程发送一个关闭信号(SIGTERM)，

                                                              等待优雅时长后，再强制删除容器，主要用于业务在退出之前完成一些善后工作

       3.1 、具体步骤   

               （1）、通过服务启动脚本去启动服务，并监听linux的SIGTERM信号，一旦监听到该信号，立马向服务进程执行kill -15命令

               （2）、微服务在启动时实现SignalHandler接口，实现handle方法，监听kill -15的linux信号

               （3）、通过服务名去msb（服务注册以及请求转发的组件）查询获取对应的pod，并查找出其他pod对应的port以及ip

               （4）、将上报到mps性能数据消费队列里面还未消费的性能数据，通过调用其他pod性能数据上报的接口进行转移。                          

                                     

      3.2、潜在的问题 :

                当pod正在退出时，如何准确的找到还没消费完的性能数据？

               （1）、性能数据可能一部分存在于下面的List集合中， 还没来得及消费
	       （2）、性能数据可能正在消费中，但是还没有入库

               （3）、上报的性能数据可能队列中，还没有开始调用processItemsList准备进行处理
	       
	 综上所述：我们还没入库的包括以上三种情况，其中第一和第三类型的数据好统计，但是第二种类型的正在消费但还未入库的数据就不好统计



     3.3、方案的优缺点                                     

                优点：（1）不需要修改当前上报逻辑

                缺点：（1）还未消费入库的性能数据难以统计，很有可能在pod退出时导致性能数据丢失。(数据可能还在队列中未被消费，或者处于消费中的某个阶段，但还未消费完)
	       
	      
	      
      方案二（kafka上报）：

      2.1、具体步骤

              （1）使用Kafka 替换掉 rest接口上报数据的机制；

              （2）mps在数据入库后，执行提交分区偏移量动作，保证pod退出时性能数据不会丢失。



      2.2、关于kafka的可靠消费（每条数据恰好只消费一次）的讨论：由于自动提交在再均衡场景会出现kafka消息的重复消费，所以一般都选用手动提交偏移量

              （1）先提交偏移量，再消费数据：

                       如果偏移量提交成功，但是数据消费失败，将会导致消息丢失

              （2）先消费数据，再提交偏移量：

                       如果数据消费成功，偏移量提交失败，将会导致消息重复消费            

         

       2.3、潜在的问题

               如果在数据入库后正好pod退出，那偏移量将提交失败，这样就会存在数据的重复消费，（性能告警必须保证不能消费重复的数据，重复数据入库由于有唯一索引

               所以重复数据入库会报错）

               解决方案1（可以作为优化方案的选项）：将性能数据和偏移量同时进行入库操作，通过数据库事物来保证整个事物操作的原子性；

                      缺点 ：整个操作比较繁琐，还得重新建表对偏移量进行存储，再均衡之后consumer得重新从数据库查询偏移量再进行消费

                       优点：能够保证消息不会重复消费。



               解决方案2（备选方案1）：以主题+分区+偏移量作为key，以性能数据作为value，构造一个全局的redis缓存，

                                mps每次消费数据之前都判断一下，该消息是否已经被消费过，如果没有消费，则消费该

                                数据，并存入缓存，否则认为是重复消息不消费。注意，要给每个key,value设置一个过期

                                时间（5分钟），让redis内存能够得到及时的释放。

                      缺点：在性能任务量比较大的场景，需要额外消费一定的redis的内存空间

                                无法保证数据入库和缓存数据这两个操作的原子性

                      优点：整个逻辑相对比较简单



              解决方案3：通过k8s的优雅停止pod机制（实现原理和性能任务同步的kafka监听原理类似），维护一个主题分区以及对应偏移量的缓存，每消费一条消息，就更新一下

                                缓存中的分区偏移量，在容器将要退出前，调用消费者的wakeup()方法，在轮询线程里面捕获wakeup异常后在finally里面将缓存里面的分区偏移量提交上去

                                 然后调用close()方法关闭kafka消费端。

                       优点：不会耗用太多redis内存，只需要不断更新一个分区到偏移量的本地缓存即可

     

     2.4、mps场景是否需要解决重复消息？ ——> 数据库是否会存在重复数据？

                首先看下我们创建索引的sql语句

                CREATE UNIQUE INDEX "idx_it_pm_it_app_ume_cpu" ON "public"."it_pm_it_app_ume_cpu" USING btree ("begintime", "dstsaving", "neid", "moid");

                从sql语句可以看出我们性能表创建了唯一索引，("begintime", "dstsaving", "neid", "moid")，所以重复索引的数据是无法入库的



      

      2.5、方案的优缺点  

                  优点：（1）解耦：uaf只需要将数据发送到kafka即可，无需关注mps进程退出时导致的服务不可用。

                            （2）代码逻辑简洁：将整个性能数据交给kafka来处理，在弹缩的分布式场景下，mps只需要保证性能数据入库后再提交偏移量，就一定不会出现

                                                             性能数据丢失的情况。

                  缺点：（1）需要修改mps接收性能数据的逻辑，由原来rest接口调用改为kafka消费者消费，但是这部分代码量的修改并不大。



    2.6、评审发现的问题及解决方案 ——> mps消费数据的方式能否是先消费，再提交偏移量?



            问题1：性能数据入库是通过管道来处理的，是多线程异步处理，如何保证最后一条性能数据处理完后再提交偏移量

            解决方案1：每次轮询之后获取轮询这批数据的数量，每消费完一条数据，就将这个数量值减一，当这个值减为0时，再将这批数据的偏移量提交上去    

            最终实现：在处理kafka消费者数据时，先创建一个CountDownLatch计数器，将计数器n置为需要处理数据的数量，每处理一条数据，就将计数器减为1，

                            减为0时就表明数据已经处理完了，kafka轮询线程开始处理下一波数据。等一批次轮询数据处理完后再提交偏移量。



            问题2：将偏移量的提交放在处理数据之后，会不会因为一次轮询的数据处理时间过长导致心跳检测超时，从而让kafka认为该消费者已经退出而触发再均衡

            背景：关于session.timeout.ms（心跳超时时间），max.poll.interval.ms（两次轮询之间的最大时间间隔），heartbeat.interval.ms（心跳检查周期）这三个参数的讨论

                     在kafka0.10.1之后的版本中，将session.timeout.ms 和 max.poll.interval.ms 解耦了。也就是说：new KafkaConsumer对象后，在while true循环中执行consumer.poll拉取消息这个过程中，

            其实背后是有2个线程的，即一个kafka consumer实例包含2个线程：一个是heartbeat 线程，另一个是processing线程，processing线程可理解为调用consumer.poll方法执行消息处理逻辑的线程，

            而heartbeat线程是一个后台线程，对程序员是"隐藏不见"的。如果消息处理逻辑很复杂，比如说需要处理5min，那么 max.poll.interval.ms可设置成比5min大一点的值。而heartbeat 线程则和上面

            提到的参数 heartbeat.interval.ms有关，heartbeat线程 每隔heartbeat.interval.ms向coordinator发送一个心跳包，证明自己还活着。只要 heartbeat线程 在 session.timeout.ms 时间内

            向 coordinator发送过心跳包，那么 group coordinator就认为当前的kafka consumer是活着的。

                     在kafka0.10.1之前，发送心跳包和消息处理逻辑这2个过程是耦合在一起的，试想：如果一条消息处理时长要5min，而session.timeout.ms=3000ms，那么等 kafka consumer处理完消息，

           group coordinator早就将consumer 移出group了，因为只有一个线程，在消息处理过程中就无法向group coordinator发送心跳包，超过3000ms未发送心跳包，group coordinator就将该

           consumer移出group了。而将二者分开，一个processing线程负责执行消息处理逻辑，一个heartbeat线程负责发送心跳包，那么：就算一条消息需要处理5min，只要底heartbeat线程在

          session.timeout.ms向group coordinator发送了心跳包，那consumer可以继续处理消息，而不用担心被移出group了。另一个好处是：如果consumer出了问题，那么在 session.timeout.ms

          内就能检测出来，而不用等到max.poll.interval.ms 时长后才能检测出来。

                      

          结论：通过上面背景知识可知，在kafka0.10.1之后的版本，心跳检查与轮询消息处理这两个超时时间已经解耦了，默认两次轮询间隔的超时时间为5分钟，完全满足我们的业务要求，所以我们只要

          解决问题1即可
	  
	  
	  
	  
	  
	  
	  1.项目中使用过的并发工具
  1.1、CountDownLatch
       使用场景：首先在主线程中初始化一个计数器n,在主线程中先创建n个线程并发解析模型，各个子线程解析完模型之后将计数器值减一，主线程等待计数器减为0后才继续往下执行创建网元的操作
	   实现原理：1、首先new CountDownLatch(n)初始化计数器的值为n；
	             2、CountDownLatch.countDown()则是调用AQS的tryReleaseShared()方法采用CAS方式将计数器的值减一，减为0时唤醒等待的主线程，主线程再去判断计数器值是否为0，为0则退出
				 3、CountDownLatch.await()则是调用AQS的tryAcquireShared方法判断计数器的值是否减为了0，如果为0则退出死循环，否则一直循环下去，从而达到阻塞当前线程的效果
				 
  1.2、ThreadLocal：
       使用场景：我们需要打印每个任务的耗时时间，扩展ThreadPoolExecutor的功能，定义一个ThreadLocal的全局变量，任务执行开始之前执行beforeExecute方法记录任务执行的开始时间，任务执行结束
                 调用afterExecute方法，首先从ThreadLocal变量中获取到任务开始时间，然后计算出任务的耗时并打印出来
       实现原理：1、initialValue()该方法需要用户重写这个方法给该ThreadLocal变量初始化值
	             2、set(value)以及get()方法
				   首先根据当前线程获取当前线程的ThreadLocalMap变量，ThreadLocalMap变量是线程类的一个全局变量,也是ThreadLocal一个内部类，每次调用set,get方法时都会首先获取当前线程的ThreadLocalMap变量，ThreadLocalMap类
				   实际上维护了一个Entry数组，set(value)方法其实就是通过ThreadLocal对象的threadLocalHashCode(每次在上次hash值上增加一个hash自增值)&(len-1)找到数组的索引位置对应的Entry，然后以ThreadLocal对象作为key，副本变量value作为value存储在Entry中
				   get()方法其实就是通过ThreadLocal对象的threadLocalHashCode&(len-1)找到数组的索引位置从而获取到这个Entry对象,然后获取这个Entry对象对应的value
				   调用get()以及set(value)方法时如果ThreadLocalMap没有创建需要先创建这个ThreadLocalMap对象
				   
				 关于ThreadLocal内存泄露的问题：
				 Entry对象里面key对应的ThreadLocal对象是弱引用，可以被gc回收，而Entry对象里面的value只要当前线程没有被销毁，就会一直存在一条强引用链，导致value无法被回收，但是在线程池场景
				 线程是不会销毁的，从而造成内存泄露。
				 解决办法：在调用set()或者get()方法时，如果当前Entry对应的key为null（也就是ThreadLocal为null），那么就将其对应的value置为null，也可以调用remove方法进行释放
				 
				 
	              
  1.3、Semaphor（信号量）
       使用场景：ipmi任务采集线程每执行一次采集都需要开启一个shell进程对设备数据进行采集，为了防止进程开启过多，可能导致服务挂掉的风险，我们通过信号量保证同时开启shell进程执行采集
	             采集的线程数目不能大于30
	   实现原理：1、首先new Semaphore(n)，传入一个信号量值n
	             2、Semaphore.aquire()方法则是调用AQS的tryAcquireShared()方法采用CAS方式获取信号量（将信号量值减1），如果获取失败，则将该线程封装成一个节点，加入队列尾部，然后进入自旋，自旋的过程中
				    将不断去判断当前节点的前置节点是否是头节点，如果是头节点就尝试去获取信号，获取成功之后将该节点设置为头节点，并退出自旋
				 3、Semaphore.release()方法则是调用AQS的tryReleaseShared()方法采用CAS的方式尝试将信号量的值加1，如果成功，唤醒头节点的后继节点
				 
  1.4、completionService
       使用场景：一款设备存在一个真实IP和两个浮动IP，真正能访问的这个IP是在这三个IP中浮动，我们在采集线程里面再通过completionService开启三个线程通过三个IP同时对这个设备进行访问并采集数据，最先能采集到
	             数据的那个线程对应的IP就是能采集到数据的那个IP，将这个IP进行缓存供下个周期粒度使用，其他两个线程由于IP不通则会请求超时
	   实现原理：将task封装成一个QueueingFuture（FutureTask的父类），task执行完之后会将其封装到一个BlockingQueue阻塞队列当中，然后调用BlockingQueue的take方法获取对应的task（这个task是Future类型的）
	             然后调用Future.get()方法就可以获取到对应的task的执行结果
				 
  1.5、Future
       使用场景：采集时有时因为设备长时间不响应导致我们采集线程一直被挂起，在采集线程里面创建一个Future任务，通过这个Future任务去获取设备返回的数据，设置这个Futrue任务的超时时间为1分钟，这样采集线程
	             最长被阻塞一分钟
				 
				 
	   实现原理：1、任务的执行：
	                1、判断当前任务状态是否为NEW，如果不是，说明任务已经执行过或取消，直接返回
					2、如果状态为NEW，则会通过unsafe类把任务执行线程保存在runner字段中，如果保存失败，直接返回
					3、执行任务
					4、任务执行成功，set保存结果，任务执行失败，setException保存异常信息，任务的状态在这里被改变
	             2、任务的取消：futrue.cancel()
	                当创建了Futrue实例，
	                任务分为三种状态：
				    1.1、还没开始执行，等待执行的任务：此时cancel()方法里面不管传入true还是false该任务都会标记为cancel，任务仍然保存在任务队列中，等到任务执行时会直接跳过
				    1.2、正在执行的任务：此时cancel()方法里面传入true将会直接中断正在执行的任务（调用interrupt方法），传入false则不会中断
				    1.3、已经执行完的任务：调用cancel()方法没有任何影响
				 当传入为true时，任务状态从NEW转化为INTERRUPTING，再转化为INTERRUPTED
				 当传入为false时，任务状态从NEW转化为CANCELLED
				 
				 3、任务结果的获取：futrue.get()
				    主要是调用awaitDone()函数，该函数是一个死循环，轮询判断任务的状态
					1、当执行的线程被中断时，调用removeWaitor方法移除等待节点WaitNode,抛出中断异常
					2、当状态为已经完成，则直接返回
					3、当状态为完成中，则通过Thread.yield()让出CPU时间
					4、如果当前线程还没有创建WaitNode等待节点，则创建等待节点插入到等待链表
					5、最后是调用LockSupport.part进入阻塞等待状态，如果设置了超时时间，需要将超时时间设置进去
					
					当任务执行完后将会调用finishCompletion方法，该方法将会调用LockSupport.unpark唤醒该线程，此时awaitDone()方法将会返回，然后调用report方法获取任务结果或异常
					
				4、FutrueTask任务状态的变化
				   1、调用构造函数
				      这个时候状态为NEW
				   2、取消操作
				      如果任务正在执行但是还没完成，此时执行发出中断，任务状态将会被置于INTERRUPTING,当任务执行完时，任务状态将会被置于INTERRUPTED (NEW --> INTERRUPTING --> INTERRUPTED)
					  如果任务还未开始执行，则直接置为取消状态CANCELLED（NEW --> CANCELLED）
				   3、执行任务操作
				      当任务执行成功（NEW --> COMPLETING --> NORMAL）,当任务执行失败（NEW --> COMPLETING --> EXCEPTIONAL）
					  
   1.6、ReenTranLock
               1.6.1、重入性：获取锁的线程可以再次获取该锁（以公平锁为例），如果不可锁重入的话，就会造成死锁
					  实现AQS的tryAcquire()方法，
					  1、首先查看同步状态state,如果没被占用（state==0）则能被当前线程占有；
					  2、如果被占有了（state!=0）则查看被占有的线程是否为当前线程，如果是，则将state自增1，表示再次获取锁成功
			          实现AQS的tryRelease()方法
					  1、每调用一次tryRelease()方法都会将同步状态自减1，当同步状态减为0时，才认为锁完全释放
			   1.6.2、公平性与非公平性:公平性是指线程在请求资源的顺序上严格遵守FIFO的特性，
					  公平锁在获取同步状态时会首先判断当前节点是否存在前驱节点，如果存在的话则说明有线程比当前的线程更早的请求资源，则获取资源失败，
					  公平锁每次都是同步队列里面的第一个节点优先获取到锁，而非公平锁则不一定
					  
					  对比：公平锁保证锁请求时间上的绝对顺序性，而非公平锁则会让刚释放锁的线程继续获取到锁，从而导致其他线程永远无法获取到锁，造成
							"饥饿"现象。但是公平锁为了保证公平性，会造成线程的频繁的上下文切换，从而增大了性能的开销
			
2、AQS原理实现
  2.1、独占式（只有一个线程能执行,例如ReentrantLock）
	   aquire()
       2.1.1、tryAcquire() 独占式获取资源，如果成功直接返回
	   2.1.2、如果失败则调用addWaiter()将线程封装成节点，通过CAS自旋的方式将节点加入到同步队列尾部：
	          进入到队列中的节点将会有如下四种状态(Node.waitStatus)：
			  1、CANCELLED:值为1，表明该节点对应的线程等待超时或被中断状态，需要从同步队列中移除该节点
			  2、SIGNAL:值为-1，后继节点的线程处于等待状态，而当前节点的线程如果释放了同步状态或者被取消，那么就会通知后继节点，让后继节点的线程能够运行
              3、CONDITION:值为-2，表明该节点处于等待队列中，当其他线程调用了Condition.signal()方法后，该节点将会从等待队列转移到同步队列，等待获取同步锁
			  4、PROPAGATE:值为-3，在共享模式下，表明该节点对应的线程处于可运行状态
			  5、INITIAL状态:值为0，代表初始化状态
			  在AQS状态判断中，waitStatus>0表示取消状态，waitStatus<0表示为有效状态
	    2.1.3、acquireQueued(Node,int):通过tryAcquire()以及addWaitor，该线程获取资源失败，则调用addWaiter()将线程封装成节点，通过CAS自旋的方式将节点
                 加入到同步队列尾部,已经放入同步队列尾部了下一步就进入到自旋状态，首先判断前驱节点是否是头节点，如果是头节点那就尝试去获取资源，
		       2.1.3.1、获取资源成功：将该节点设置为头节点，并将前驱节点的next节点置为null，直接返回
			   2.1.3.2、获取资源失败：如果前驱的状态是SIGNAL状态，那就可以安心休息了，直接返回，如果前驱节点状态大于0，表明前驱节点放弃了，就一直往前找，直到找到最近的一个正常的状态，
			            并排在他后面，如果前驱节点的状态不为SIGNAL，则将前驱节点的状态设置为SIGNAL，告诉前驱节点释放资源后及时通知自己。
						最后调用LockSupport.park()方法使线程进入到等待状态。
		release()
		2.1.4、调用tryRelease释放资源，如果已经彻底释放资源(state = 0),则返回true,否则返回false
		       释放成功之后调用LockSupport.unpark(thread)唤醒头节点的下一个有效节点（waitstatus<=0）所对应的线程
			   
   2.2、共享式（多个线程可以同时执行，CountDownLatch,Semaphor）
		aquireShared()
		2.3.1、1、tryAcuireShare()尝试获取资源，成功则直接返回；
			   2、失败则进入同步队列，在保证前置节点为SIGNAL状态之后调用park()方法让线程进入等待状态，
                              直到被前置节点unpark()之后被唤醒并重新尝试获取资源，
			   
			   其实和独占式的acquire()方法的原理大同小异，只不过多了一个当前节点成功获取到资源以后，如果资源还有剩余，则唤醒后继的节点
			   
		releaseShared()
		2.3.2、1、tryreleaseShared()尝试释放资源，成功后调用unpark()方法唤醒后继节点
		      与独占式不同的时候，独占式只允许一个线程获取到资源，所以完全释放资源后，当state==0后才会唤醒后继节点，而共享式则允许多个线程同时获取资源，这就需要每当有线程释放资源时，需要尝试唤醒后继的线程去获取资源
			  
			  
			  
			   
		
              1、top命令查看进程对应所有线程对应的CPU占用情况，发现有个线程经常占用率达到百分之70,80.找到线程对应的线程ID
2、通过jstack命令打印当前进程对应的线程栈信息。将找到的十进制线程ID转化为16进制的线程ID，并在线程栈中去查找对应的线程调用栈
3、发现就是我们有一个逻辑就是，5分钟定时任务在每条任务下发时会将上次缓存的所有执行的任务耗时进行排序，然后通过一个可配置的系统变量找到一个临界值，
	通过这个临界值来将这个线程分配到快，慢两个线程池中。我们大概有两万个任务，相当于每下发一个任务就对这两万个缓存值进行排序，每5分钟要排序两万次
	，这就是导致cpu冲高的根本原因
4、解决办法：将计算任务耗时临界值的逻辑通过一个定时任务来处理，每一秒钟来对这两万个缓存数据进行排序，寻找出临界值




我们团队主要负责通信硬件(例如交换机，路由器)等设备的性能指标监控(比如说cpu，内存，端口等一些指标的监控。)，一旦发现硬件设备性能指标出现异常，
及时上报告警
1、首先在itm-mps界面上导入模型文件，模型文件(一类模型文件对应一类设备，比如说有华为服务器，HP服务器，中兴服务器，这就是三类设备，对应的就是三种模型文件)：模型文件是个zip包，主要是存放了一些xml(xml文件主要是用来定义我们需要获取设备哪些性能指标，以及这些指标对应的获取方式，我们最终是将xml文件里面的信息解析到缓存中，后面任务要对具体设备指标进行采集时会从缓存中进行获取)，excel(excel文件主要是存储某类设备对应的信息字段，就比如说中兴服务器，它就有Ip，端口，用户名，密码等，我们称为资源信息。还包括性能指标对应的信息字段，比如说中兴服务器有一个CPU性能指标，这个CPU性能指标又包括CPU使用率，CPU空闲率，CPU等待率，然后又有个内存指标又包含有内存使用率，内存交换请求数等等，我们称之为性能信息，解析完excel之后我们会将生成的资源信息通过调用rest接口的方式存储在资源组件对应的数据库里面，(建一张表，中兴服务器的表)，生成的性能信息我们会存储在性能组件对应的数据库中，并且在我们自己数据库里面建一张性能表(就比如建一张中兴服务器CPU指标的表)),js(js主要是对一些性能)等文件

2、设备信息
在解析完模型文件后，我们开始在itm-mps界面上添加设备实例(就比如说有一台中兴服务器，它的ip是1.2.3.4，端口是161，用户名，密码等等字段信息，我们在界面填完这些设备信息之后，我们最终会调资源接口在资源组件中将这个设备信息记录存储到之前创建的中兴服务器的表当中)。添加成功之后会同步到itm-uaf

3、任务
在界面上添加完资源信息之后，我们开始在性能界面创建任务，你要创建哪个设备对应的哪个性能指标的任务(设备信息从资源组件获取,性能指标信息之前在解析模型文件的时候就已经在性能进行了存储，直接获取就行）,任务创建完成之后首先通过kafaka同步到itm-mps，经过稍加处理之后，再通过rest接口的方式同步到itm-uaf，itm-uaf就是一个采集器，对各个任务对应的性能指标数据进行采集获取。所有任务同步到itm-uaf之后会放在一个5分钟周期的定时任务里面，然后每隔5分钟执行一次，每次执行的时候是首先将任务放在线程池中，由线程池进行调度，执行的过程中任务会首先获取设备信息(设备添加完后除了会将设备信息存储在资源组件，还会以缓存的形式存储到itm-uaf)，然后从xml文件里面生成的缓存里面获取当前任务具体的采集方式（具体就是采用什么协议什么方式进行采集），采集到数据之后对数据进行处理封装，处理完了之后将数据丢到另外一个上报的线程池里面，由这个上报的线程池将数据通过kafka的方式上报给itm-mps，itm-mps会对性能数据进行判断，如果异常则上报告警，正常则存储在数据库中。

4、我们这两个微服务有两个应用场景
   一个场景我们就几百个设备，几千个任务，数据最终是存储在我们的数据库（itm-mps），这种场景下性能压力较小
  
   还有一个场景我们有三千多个设备，两万多个任务，这种场景我们只负责任务的创建，采集以及上报，但不负责数据的存储（只单独使用了itm-uaf），这种场景下性能压力较大，itm-uaf采用水平扩展的方式来解决
   
   
5、k8s之Pod的水平字自动伸缩
   随着工作负载的不断加大，我们的应用程序需要不断的适应不同工作负载下的性能压力，这个时候我们需要使用k8s的pod水平自动伸缩技术来对pod的数据进行水平伸缩，性能压力较大是我们扩展pod数量，性能压力
   较小时我们减小pod数量，我们会预先设置根据哪些指标进行水平伸缩，并确定一个阈值，比如说cpu使用率，内存使用率超过某个阈值比如说70%时就会水平扩展出多个pod，当指标低于某个阈值时又会有pod消亡。


			
		
		
		
1.模型
  问题：在单pod场景下模型上传解析完了将解析完将生产的xml等文件放在runtime/unzip目录下之后可以了，但是在多pod场景下runtime/unzip路径是隔离开的，这个时候就要保证模型文件
  能够在多个pod上进行解析。
   解决办法：
   方案一：uaf在模型加载时，会对unzip目录下的excel，xml文件进行解析并生成本地缓存，为了保证多pod分布式场景下缓存的一致性，使用redis替换本地缓存，存储模型相关信息
   缺点：uaf在加载模型时，依赖的缓存过多，流程繁琐不易扩展，需要花费一定的时间对代码进行重构，再对缓存进行替换
   
   方案二：通过监听模型文件是否发生变化，使得模型在加载时每个pod都能感知得到，从而保证本地缓存的一致
   相对于方案一的优点：不需要对原有代码进行重构，直接在原有代码基础上进行扩展新增相应逻辑即可
   
   方案二的具体实现步骤：
   1、服务启动时，开启模型文件夹监听，监听模型文件夹文件是否发生变化
   2、在当前pod完成模型文件的上传解析或删除操作后，将会往uaf插入一条记录，该记录redis key是UAF_MODEL_HANDLE,选择hash结构进行存储，key是模型唯一标志符，value表示这个模型处理的一个状态，有如下字段
      2.1、moc：模型的唯一标识符
	  2.2、typeId：模型的操作类型，分为上传以及删除两个操作
	  2.3、podNum: 表示当前pod的数量
      2.4、podCompletedNum：完成模型处理的pod的数量
   3、其他pod监听到模型文件夹发生变化以后，去redis查询该记录，并判断是哪个模型执行了具体哪个操作，然后在该pod上执行相应的操作，执行完之后将该记录的podCompletedNum加一
   4、发起模型上传和卸载操作后的pod在执行完相应操作后，会启动一个定时任务，对redis key是UAF_MODEL_HANDLE的这条记录进行检查，当该记录的podNum与podCompletedNum相等时，则认为所有
      pod都完成了模型处理操作，这条请求也就结束了
   
   方案三：通过redis的订阅发布逻辑保证一个pod上模型的上传加载以及删除操作能够通知到其他的pod，从而保证本地缓存的一致
   相对于方案二的优点：规避了方案二繁琐的代码逻辑，将整个模型的上传加载以及删除动作的感知过程全部通过第三方redis的订阅发布逻辑来实现
            1、每个pod在启动的时候都订阅一个模型变化的主题（模型的上传解析和删除），收到模型上传的pod解析完模型之后将模型文件放到各个pod共享的一个持久化目录中，
            2、然后该pod向频道发送模型变化的消息，然后频道将该消息发给各个pod进行模型解析（当前pod收到消息之后不做任何处理）。
			3、当然第一个接收到模型解析的那个pod必须等待其它pod模型解析完之后，整个请求才算结束。这个我们是使用redis分布式闭锁来解决的。
			
  技术亮点：除了使用redis订阅发布机制以外，还使用redis分布式闭锁	
  开发出现的问题：有两个订阅逻辑，一个是当前pod监听模型变化的消息，还有一个是其他pod监听当前pod模型变化的消息。
                  一旦当前pod监听到模型变化的消息时就会通知其他pod进行模型解析的操作，而其他pod在进行模型解析完时又会通知除它以外的其他pod进行模型解析操作。这样就会导致模型解析进入了一个死循环
				  最终导致模型上传解析接口超时。
  解决办法：传入一个bool型标志位，如果为true则表明是从接口接收到模型上传的请求，则需要发布该消息给订阅者，而false则表明是其他pod通过发布模型变化的消息导致的模型上传解析，模型解析完后则无需再将
            该请求消息发布给其他pod
  备注：模型变化的消息实体包括三个字段：moc,ip,operate（delete,add）
        通过moc+operate能够获取到具体对哪个模型进行哪种操作，通过这个值可以唯一的获取到一个CountDownLatch
		ip用来判断是否是当前pod发送过来的模型变化消息，如果是则不需要处理。
			
2.网元：缓存主要用的hash的数据结构，Redis的key是itm-uaf:ne:instance_info
  hash里面主要是以oid作为key，唯一确定一个网元
  
3.性能任务: 缓存主要用的hash的数据结构，Redis的key是itm-uaf:pm:task_info，这是一个任务的全局缓存
  hash里面主要是以taskID作为key,唯一确定一条任务
  
4.pod信息：缓存主要用的hash的数据结构，Redis的key是itm-uaf:pod:instance_info，这是一个pod的缓存
  hash里面主要是以ip作为key,唯一确定一个pod的信息
  pod信息主要有如下字段：
  private String podIpAddress; pod对应的ip
  private boolean isMaster;  该pod是否是主
  private long syncTime;     该pod中任务重新分配，需要将其设置为当前时间戳，否则为-1。
  private List<TaskInfo> initTaskList; 该pod对应的任务列表。
  
  
  我们每隔15s判断一次缓存中的pod数量与当前pod数量是否一致，不一致时则表明itm-uaf微服务进行了伸缩（整个过程需要通过redis分布式锁来保证整个处理过程只有一个pod在进行处理）
  1：当前pod数量大于缓存中pod数量，则表明有pod扩展，则直接将任务按照轮询的算法进行重新分配，然后将缓存里面的pod信息（任务列表更新，同步时间戳设置为当前时间戳）
  2：当前pod数量小于缓存中pod数量，则表明有pod消亡，则直接将该pod上正在执行的任务转移到其它pod上执行，防止任务存在粒度丢失，并将任务按照轮询的算法进行重新分配。
     然后将缓存里面的pod信息（任务列表更新，同步时间戳设置为当前时间戳）
	
  如何判断正在执行的任务？
  任务执行之前将其放入到缓存，任务执行完了将其从缓存中去除，缓存中剩余的就是正在执行的任务。
	 
  我们每隔15s从缓存中获取当前pod信息并进行判断，如果不为-1，则表明进行过任务重分配，这个时候我们需要将当前pod对应的任务删掉，
  并从该pod信息中获取到更新过的任务列表进行创建，然后件时间戳设置为-1
  
  
  
  
  
  http与https的区别
1、HTTPS更安全：HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，要比HTTP协议的信息明文传输安全；
原文地址：http://seo.yechangliang.com/post-550.html

2、端口不同：HTTP使用的是大家最常见的80端口，而HTTPS连接使用的是443端口；
原文地址：http://seo.yechangliang.com/post-550.html
                

                      

                                  



	       
       
  
